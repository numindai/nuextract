{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbe8b1f6-ec66-4fbc-a9f8-774acd8a255a",
   "metadata": {},
   "source": [
    "# NuExtract 2 Inference\n",
    "\n",
    "In this notebook we will provide examples of how to use the NuExtract 2.0 models for inference.\n",
    "\n",
    "First, let's load a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a561e38e-58e5-4f7e-80ee-eabc2d05e49a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "InternLM2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"numind/NuExtract-2-2B\"\n",
    "# model_name = \"numind/NuExtract-2-4B\"\n",
    "# model_name = \"numind/NuExtract-2-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             trust_remote_code=True, \n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             attn_implementation=\"flash_attention_2\"\n",
    "                                            ).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5c83db-9019-421f-8f8c-ad86cc037218",
   "metadata": {},
   "source": [
    "Next, we need to include some utility code to preprocess images to be compatible with InternVL-2.5 (base model that NuExtract 2.0 is built on). We will also include a function `nuextract_generate()` to handle our inference process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16c7a460-6201-4236-a645-ff2f845a6ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "IMG_START_TOKEN='<img>'\n",
    "IMG_END_TOKEN='</img>'\n",
    "IMG_CONTEXT_TOKEN='<IMG_CONTEXT>'\n",
    "\n",
    "def nuextract_generate(model, tokenizer, prompts, generation_config, pixel_values_list=None, num_patches_list=None):\n",
    "    \"\"\"\n",
    "    Generate responses for a batch of NuExtract inputs.\n",
    "    Support for multiple and varying numbers of images per prompt.\n",
    "    \n",
    "    Args:\n",
    "        model: The vision-language model\n",
    "        tokenizer: The tokenizer for the model\n",
    "        pixel_values_list: List of tensor batches, one per prompt\n",
    "                          Each batch has shape [num_images, channels, height, width] or None for text-only prompts\n",
    "        prompts: List of text prompts\n",
    "        generation_config: Configuration for text generation\n",
    "        num_patches_list: List of lists, each containing patch counts for images in a prompt\n",
    "        \n",
    "    Returns:\n",
    "        List of generated responses\n",
    "    \"\"\"\n",
    "    img_context_token_id = tokenizer.convert_tokens_to_ids(IMG_CONTEXT_TOKEN)\n",
    "    model.img_context_token_id = img_context_token_id\n",
    "    \n",
    "    # Replace all image placeholders with appropriate tokens\n",
    "    modified_prompts = []\n",
    "    total_image_files = 0\n",
    "    total_patches = 0\n",
    "    image_containing_prompts = []\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        # check if this prompt has images\n",
    "        has_images = (pixel_values_list and\n",
    "                      idx < len(pixel_values_list) and \n",
    "                      pixel_values_list[idx] is not None and \n",
    "                      isinstance(pixel_values_list[idx], torch.Tensor) and\n",
    "                      pixel_values_list[idx].shape[0] > 0)\n",
    "        \n",
    "        if has_images:\n",
    "            # prompt with image placeholders\n",
    "            image_containing_prompts.append(idx)\n",
    "            modified_prompt = prompt\n",
    "            \n",
    "            patches = num_patches_list[idx] if (num_patches_list and idx < len(num_patches_list)) else []\n",
    "            num_images = len(patches)\n",
    "            total_image_files += num_images\n",
    "            total_patches += sum(patches)\n",
    "            \n",
    "            # replace each <image> placeholder with image tokens\n",
    "            for i, num_patches in enumerate(patches):\n",
    "                image_tokens = IMG_START_TOKEN + IMG_CONTEXT_TOKEN * model.num_image_token * num_patches + IMG_END_TOKEN\n",
    "                modified_prompt = modified_prompt.replace('<image>', image_tokens, 1)\n",
    "        else:\n",
    "            # text-only prompt\n",
    "            modified_prompt = prompt\n",
    "        \n",
    "        modified_prompts.append(modified_prompt)\n",
    "    \n",
    "    # process all prompts in a single batch\n",
    "    tokenizer.padding_side = 'left'\n",
    "    model_inputs = tokenizer(modified_prompts, return_tensors='pt', padding=True)\n",
    "    input_ids = model_inputs['input_ids'].to(model.device)\n",
    "    attention_mask = model_inputs['attention_mask'].to(model.device)\n",
    "    \n",
    "    eos_token_id = tokenizer.convert_tokens_to_ids(\"<|im_end|>\\n\".strip())\n",
    "    generation_config['eos_token_id'] = eos_token_id\n",
    "    \n",
    "    # prepare pixel values\n",
    "    flattened_pixel_values = None\n",
    "    if image_containing_prompts:\n",
    "        # collect and concatenate all image tensors\n",
    "        all_pixel_values = []\n",
    "        for idx in image_containing_prompts:\n",
    "            all_pixel_values.append(pixel_values_list[idx])\n",
    "        \n",
    "        flattened_pixel_values = torch.cat(all_pixel_values, dim=0)\n",
    "        print(f\"Processing batch with {len(prompts)} prompts, {total_image_files} actual images, and {total_patches} total patches\")\n",
    "    else:\n",
    "        print(f\"Processing text-only batch with {len(prompts)} prompts\")\n",
    "    \n",
    "    # generate outputs\n",
    "    outputs = model.generate(\n",
    "        pixel_values=flattened_pixel_values,  # will be None for text-only prompts\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        **generation_config\n",
    "    )\n",
    "    \n",
    "    # Decode responses\n",
    "    responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d739f582-16ad-4e52-b012-429757945a84",
   "metadata": {},
   "source": [
    "## Preparing Model Inputs\n",
    "\n",
    "Before using the model, we also need to make sure our prompts are properly formatted to work with NuExtract. NuExtract expects all input information to come as a single user chat prompt, formatted as follows:\n",
    "\n",
    "```python\n",
    "f\"\"\"\n",
    "# Template:\n",
    "{template}\n",
    "# Context:\n",
    "{text}\n",
    "\"\"\"\n",
    "```\n",
    "and if in-context examples are provided:\n",
    "```python\n",
    "f\"\"\"\n",
    "# Template:\n",
    "{template}\n",
    "# Examples\n",
    "## Input:\n",
    "{input1}\n",
    "## Output:\n",
    "{output1}\n",
    "## Input:\n",
    "{input2}\n",
    "## Output:\n",
    "{output2}\n",
    "# Context:\n",
    "{text}\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "If you are working with image inputs, simply use `\"<image>\"` placeholders for `text`, `output1`, etc. The code below will then inject tokens representing the actual image content in the location of these placeholders.\n",
    "\n",
    "The following functions will make this formatting more convenient for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58b017a4-4802-4afe-b2b4-8f034f572c24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def construct_message(text, template, examples=None):\n",
    "    \"\"\"\n",
    "    Construct the individual NuExtract message texts, prior to chat template formatting.\n",
    "    \"\"\"\n",
    "    # add few-shot examples if needed\n",
    "    if examples is not None and len(examples) > 0:\n",
    "        icl = \"# Examples:\\n\"\n",
    "        for row in examples:\n",
    "            icl += f\"## Input:\\n{row['input']}\\n## Output:\\n{row['output']}\\n\"\n",
    "    else:\n",
    "        icl = \"\"\n",
    "        \n",
    "    return f\"\"\"# Template:\\n{template}\\n{icl}# Context:\\n{text}\"\"\"\n",
    "\n",
    "def prepare_inputs(messages, image_paths, tokenizer, device='cuda', dtype=torch.bfloat16):\n",
    "    \"\"\"\n",
    "    Prepares multi-modal input components (supports multiple images per prompt).\n",
    "    \n",
    "    Args:\n",
    "        messages: List of input messages\n",
    "        image_paths: List where each element is either None (for text-only) or a list of image paths\n",
    "        tokenizer: The tokenizer to use for applying chat templates\n",
    "        device: Device to place tensors on ('cuda', 'cpu', etc.)\n",
    "        dtype: Data type for image tensors (default: torch.bfloat16)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains 'prompts', 'pixel_values_list', and 'num_patches_list' ready for the model\n",
    "    \"\"\"\n",
    "    # Make sure image_paths list is at least as long as messages\n",
    "    if len(image_paths) < len(messages):\n",
    "        # Pad with None for text-only messages\n",
    "        image_paths = image_paths + [None] * (len(messages) - len(image_paths))\n",
    "    \n",
    "    # Process images and collect patch information\n",
    "    loaded_images = []\n",
    "    num_patches_list = []\n",
    "    for paths in image_paths:\n",
    "        if paths and isinstance(paths, list) and len(paths) > 0:\n",
    "            # Load each image in this prompt\n",
    "            prompt_images = []\n",
    "            prompt_patches = []\n",
    "            \n",
    "            for path in paths:\n",
    "                # Load the image\n",
    "                img = load_image(path).to(dtype=dtype, device=device)\n",
    "                \n",
    "                # Ensure img has correct shape [patches, C, H, W]\n",
    "                if len(img.shape) == 3:  # [C, H, W] -> [1, C, H, W]\n",
    "                    img = img.unsqueeze(0)\n",
    "                    \n",
    "                prompt_images.append(img)\n",
    "                # Record the number of patches for this image\n",
    "                prompt_patches.append(img.shape[0])\n",
    "            \n",
    "            loaded_images.append(prompt_images)\n",
    "            num_patches_list.append(prompt_patches)\n",
    "        else:\n",
    "            # Text-only prompt\n",
    "            loaded_images.append(None)\n",
    "            num_patches_list.append([])\n",
    "    \n",
    "    # Create the concatenated pixel_values_list\n",
    "    pixel_values_list = []\n",
    "    for prompt_images in loaded_images:\n",
    "        if prompt_images:\n",
    "            # Concatenate all images for this prompt\n",
    "            pixel_values_list.append(torch.cat(prompt_images, dim=0))\n",
    "        else:\n",
    "            # Text-only prompt\n",
    "            pixel_values_list.append(None)\n",
    "    \n",
    "    # Format messages for the model\n",
    "    if all(isinstance(m, str) for m in messages):\n",
    "        # Simple string messages: convert to chat format\n",
    "        batch_messages = [\n",
    "            [{\"role\": \"user\", \"content\": message}] \n",
    "            for message in messages\n",
    "        ]\n",
    "    else:\n",
    "        # Assume messages are already in the right format\n",
    "        batch_messages = messages\n",
    "    \n",
    "    # Apply chat template\n",
    "    prompts = tokenizer.apply_chat_template(\n",
    "        batch_messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'prompts': prompts,\n",
    "        'pixel_values_list': pixel_values_list,\n",
    "        'num_patches_list': num_patches_list\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e918d64e-bc31-4d68-8ef5-2d485b5e5745",
   "metadata": {},
   "source": [
    "## Inference\n",
    "### Basic Example\n",
    "\n",
    "Now we are ready to run the model!\n",
    "\n",
    "Let's start with a basic text-only example, where we want to extract peoples' names from a short text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1cad1ec-4753-4064-aca0-c89fec4b5bfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"{\"names\": [\"verbatim-string\"]}\"\"\"\n",
    "text = \"John went to the restaurant with Mary. James went to the cinema.\"\n",
    "\n",
    "# prepare the user message content\n",
    "input_messages = [construct_message(text, template)]\n",
    "\n",
    "# prepare actual input content\n",
    "input_content = prepare_inputs(\n",
    "    messages=input_messages,\n",
    "    image_paths=[], # we give empty list of image paths because this is text-only\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0e4f92-94dd-4b37-8772-591246f9b71a",
   "metadata": {},
   "source": [
    "Our NuExtract message is now formatted in standard chat template formatting, which is what is given directly to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42bd5add-b48a-44e4-9843-2f187e768678",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><|im_start|>user\n",
      "# Template:\n",
      "{\"names\": [\"verbatim-string\"]}\n",
      "# Context:\n",
      "John went to the restaurant with Mary. James went to the cinema.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(input_content[\"prompts\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0979a7-ea46-4d25-a838-baeaeeca6ee5",
   "metadata": {},
   "source": [
    "The other input items are empty in this case because they are only relevant when we're using images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3493013-313f-4318-a813-d492732bb0c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['prompts', 'pixel_values_list', 'num_patches_list'])\n",
      "[None]\n",
      "[[]]\n"
     ]
    }
   ],
   "source": [
    "print(input_content.keys())\n",
    "print(input_content[\"pixel_values_list\"])\n",
    "print(input_content[\"num_patches_list\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e300775-48a9-42fb-9c92-3e1ceb530101",
   "metadata": {},
   "source": [
    "Now let's actually run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed374bf3-1af1-4a3e-a9eb-9e2e7ca113b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text-only batch with 1 prompts\n",
      "{\"names\": [\"John\", \"Mary\", \"James\"]}\n"
     ]
    }
   ],
   "source": [
    "# we choose greedy sampling here, which works well for most information extraction tasks\n",
    "generation_config = {\"do_sample\": False, \"num_beams\": 1, \"max_new_tokens\": 2048}\n",
    "\n",
    "# run the model\n",
    "with torch.no_grad():\n",
    "    result = nuextract_generate(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompts=input_content['prompts'], # input prompts containing template, etc.\n",
    "        pixel_values_list=input_content['pixel_values_list'], # image information\n",
    "        num_patches_list=input_content['num_patches_list'], # number of patches used per image\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "for y in result:\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18002f2-447a-45c2-90f6-0ff5a63c5dc2",
   "metadata": {},
   "source": [
    "### In-Context Examples\n",
    "\n",
    "Sometimes the model might not perform as well as we want because our task is challenging or involves some degree of ambiguity. Alternatively, we may want the model to follow some specific formatting, or just give it a bit more help. In cases like this it can be valuable to provide \"in-context examples\" to help NuExtract better understand the task.\n",
    "\n",
    "To do so, we can provide a list `examples` to `construct_message` which contain dictionaries of input/output pairs. In the example below, we show to the model that we want the extracted names to be in captial letters with +s on either side (for the sake of illustration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "146baa04-2206-4eca-86bd-66f94924e293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"{\"names\": [\"verbatim-string\"], \"female_names\": [\"verbatim-string\"]}\"\"\"\n",
    "text = \"John went to the restaurant with Mary. James went to the cinema.\"\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Stephen is the manager at Susan's store.\",\n",
    "        \"output\": \"\"\"{\"names\": [\"+STEPHEN+\", \"+SUSAN+\"], \"female_names\": [\"+SUSAN+\"]}\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "input_messages = [construct_message(text, template, examples)]\n",
    "\n",
    "input_content = prepare_inputs(\n",
    "    messages=input_messages,\n",
    "    image_paths=[],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2561cc-a662-4898-b226-41d12394fdb4",
   "metadata": {},
   "source": [
    "We can see below that the in-context example has now been included in the model prompt, specifically between the template and context components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79b10e44-5bb4-40eb-af74-e013897948ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><|im_start|>user\n",
      "# Template:\n",
      "{\"names\": [\"verbatim-string\"], \"female_names\": [\"verbatim-string\"]}\n",
      "# Examples:\n",
      "## Input:\n",
      "Stephen is the manager at Susan's store.\n",
      "## Output:\n",
      "{\"names\": [\"+STEPHEN+\", \"+SUSAN+\"], \"female_names\": [\"+SUSAN+\"]}\n",
      "# Context:\n",
      "John went to the restaurant with Mary. James went to the cinema.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(input_content[\"prompts\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f1364bf-2588-4935-a478-7a85b87866fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text-only batch with 1 prompts\n",
      "{\"names\": [\"+JOHN+\", \"+MARY+\", \"+JAMES+\"], \"female_names\": [\"+MARY+\"]}\n"
     ]
    }
   ],
   "source": [
    "generation_config = {\"do_sample\": False, \"num_beams\": 1, \"max_new_tokens\": 2048}\n",
    "\n",
    "with torch.no_grad():\n",
    "    result = nuextract_generate(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompts=input_content['prompts'],\n",
    "        pixel_values_list=input_content['pixel_values_list'],\n",
    "        num_patches_list=input_content['num_patches_list'],\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "for y in result:\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0c3ecf-80ee-40b7-b240-12b5a9e10042",
   "metadata": {},
   "source": [
    "To get even better performance, add multiple in-context examples to your input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99faf117-18b7-4b5e-8cd0-b6f8941db581",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Image Inputs\n",
    "\n",
    "If we want to give image inputs to NuExtract, instead of text, we simply use the placeholder `\"<image>\"` anywhere we would normally have texts (main input and ICL examples). Then we provide a list of image paths to `prepare_inputs()`. This should be a list of lists, so that multiple images can be injected when using ICL examples. The order of the image paths should match order of appearance in the prompt (i.e. ICL inputs first, with the main input at the end).\n",
    "\n",
    "In this example, we give an image of a receipt (`1.jpg`) and ask the model to extract the name of the store. We also provide an ICL example of a receipt from Walmart (`0.jpg`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd11e855-594b-4fd7-af01-7695a65b75a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"{\"store\": \"verbatim-string\"}\"\"\"\n",
    "text = \"<image>\"\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"<image>\",\n",
    "        \"output\": \"\"\"{\"store\": \"WALMART\"}\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "input_messages = [construct_message(text, template, examples)]\n",
    "\n",
    "images = [\n",
    "    [\"0.jpg\", \"1.jpg\"]\n",
    "]\n",
    "\n",
    "input_content = prepare_inputs(\n",
    "    messages=input_messages,\n",
    "    image_paths=images,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14a8e8a-a091-410a-a964-9d275b936e75",
   "metadata": {},
   "source": [
    "Just like in the text-only case above, our in-context example has been included in the prompt before the main context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba00c8d6-56e2-43fc-a646-61f693c6b51a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><|im_start|>user\n",
      "# Template:\n",
      "{\"store\": \"verbatim-string\"}\n",
      "# Examples:\n",
      "## Input:\n",
      "<image>\n",
      "## Output:\n",
      "{\"store\": \"WALMART\"}\n",
      "# Context:\n",
      "<image><|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(input_content[\"prompts\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515e4113-1a78-4f3f-a3bd-6524fa50e23d",
   "metadata": {},
   "source": [
    "Now that we are using images, the values of `pixel_values_list` and `num_patches_list` are actually meaningful. Below we can see that the first image (`0.jpg`) has been broken into 7 patches, and the second (`1.jpg`) into 3. This is comfirmed by looking at the size of the tensor in `pixel_values_list` -- 10 patches * 3 (RGB) * 448*448 (patch dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f38c3ea-c265-49bf-a29e-1a4c1b17ddc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['prompts', 'pixel_values_list', 'num_patches_list'])\n",
      "torch.Size([10, 3, 448, 448])\n",
      "[[7, 3]]\n"
     ]
    }
   ],
   "source": [
    "print(input_content.keys())\n",
    "print(input_content[\"pixel_values_list\"][0].shape)\n",
    "print(input_content[\"num_patches_list\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbed800c-519b-4ee1-90d9-7c27e383d904",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch with 1 prompts, 2 actual images, and 10 total patches\n",
      "{\"store\": \"TRADER JOE'S\"}\n"
     ]
    }
   ],
   "source": [
    "generation_config = {\"do_sample\": False, \"num_beams\": 1, \"max_new_tokens\": 2048}\n",
    "\n",
    "with torch.no_grad():\n",
    "    result = nuextract_generate(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompts=input_content['prompts'],\n",
    "        pixel_values_list=input_content['pixel_values_list'],\n",
    "        num_patches_list=input_content['num_patches_list'],\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "for y in result:\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6beb77f-2de5-4846-b244-2cda27f5f553",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Multi-Modal Batches\n",
    "\n",
    "Finally, we can run batched inference over a list of input examples, regardless of whether they contain text, images, and/or ICL examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "947b8090-236c-4821-b295-5a8fb5247943",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch with 4 prompts, 3 actual images, and 17 total patches\n",
      "{\"store_name\": \"WAL*MART\"}\n",
      "{\"store_name\": \"Trader Joe's\"}\n",
      "{\"names\": [\"John\", \"Mary\", \"James\"]}\n",
      "{\"names\": [\"JOHN\", \"MARY\", \"JAMES\"], \"female_names\": [\"MARY\"]}\n"
     ]
    }
   ],
   "source": [
    "inputs = [\n",
    "    # image input with no ICL examples\n",
    "    {\n",
    "        \"text\": \"<image>\",\n",
    "        \"template\": \"\"\"{\"store_name\": \"verbatim-string\"}\"\"\",\n",
    "        \"examples\": None,\n",
    "    },\n",
    "    # image input with 1 ICL example\n",
    "    {\n",
    "        \"text\": \"<image>\",\n",
    "        \"template\": \"\"\"{\"store_name\": \"verbatim-string\"}\"\"\",\n",
    "        \"examples\": [\n",
    "            {\n",
    "                \"input\": \"<image>\",\n",
    "                \"output\": \"\"\"{\"store_name\": \"Walmart\"}\"\"\",\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    # text input with no ICL examples\n",
    "    {\n",
    "        \"text\": \"John went to the restaurant with Mary. James went to the cinema.\",\n",
    "        \"template\": \"\"\"{\"names\": [\"verbatim-string\"]}\"\"\",\n",
    "        \"examples\": None,\n",
    "    },\n",
    "    # text input with ICL example\n",
    "    {\n",
    "        \"text\": \"John went to the restaurant with Mary. James went to the cinema.\",\n",
    "        \"template\": \"\"\"{\"names\": [\"verbatim-string\"], \"female_names\": [\"verbatim-string\"]}\"\"\",\n",
    "        \"examples\": [\n",
    "            {\n",
    "                \"input\": \"Stephen is the manager at Susan's store.\",\n",
    "                \"output\": \"\"\"{\"names\": [\"STEPHEN\", \"SUSAN\"], \"female_names\": [\"SUSAN\"]}\"\"\"\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "input_messages = [\n",
    "    construct_message(\n",
    "        x[\"text\"], \n",
    "        x[\"template\"], \n",
    "        x[\"examples\"]\n",
    "    ) for x in inputs\n",
    "]\n",
    "\n",
    "images = [\n",
    "    [\"0.jpg\"],\n",
    "    [\"0.jpg\", \"1.jpg\"],\n",
    "    None,\n",
    "    None\n",
    "]\n",
    "\n",
    "input_content = prepare_inputs(\n",
    "    messages=input_messages,\n",
    "    image_paths=images,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_config = {\"do_sample\": False, \"num_beams\": 1, \"max_new_tokens\": 2048}\n",
    "\n",
    "with torch.no_grad():\n",
    "    result = nuextract_generate(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompts=input_content['prompts'],\n",
    "        pixel_values_list=input_content['pixel_values_list'],\n",
    "        num_patches_list=input_content['num_patches_list'],\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "for y in result:\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf456dcc-52ce-495d-872f-a641c6f40f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
