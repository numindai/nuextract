{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbe8b1f6-ec66-4fbc-a9f8-774acd8a255a",
   "metadata": {},
   "source": [
    "# NuExtract 2.0 Inference\n",
    "\n",
    "In this notebook we will provide examples of how to use the NuExtract 2.0 models for inference.\n",
    "\n",
    "First, let's load a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a561e38e-58e5-4f7e-80ee-eabc2d05e49a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5b6d7ba7be4ab68b7bfe6dd5c17ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/765 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984edab1bafc465c9634952556834b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/6.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599aeb2b343641f6bb07e45a1dc1113a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85fa167997a4482388b70cd55d7c8bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228c036f496b4f4b8b569bd88e9ff052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f454191b8c94eab827cfe60bed0485a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5c6c8665d54b949eed241893d9d7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea2ee61bbc749d4b38a28f747e06700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/2.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830aa6db0361428e9a5ea5cf5f92a3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199ca54809064054a6d53d448bfeb00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/57.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f1da13b61143e19a0a5faee62578da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf1e67ee97f40149c7bcba033121660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba3753cce1c4ac3b2962b0956a746c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1179b957f864bf5ad0c60dc2a9d7da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.69G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84eee7bc78c64fe798b0235ae6874a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45dee237d392430f8e9d397285af53d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3292b0032943eca44101e5258bda9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/260 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "model_name = \"numind/NuExtract-2.0-2B\"\n",
    "# model_name = \"numind/NuExtract-2.0-8B\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name, \n",
    "                                          trust_remote_code=True, \n",
    "                                          padding_side='left',\n",
    "                                          use_fast=True)\n",
    "model = AutoModelForVision2Seq.from_pretrained(model_name, \n",
    "                                               trust_remote_code=True, \n",
    "                                               torch_dtype=torch.bfloat16,\n",
    "                                               attn_implementation=\"flash_attention_2\",\n",
    "                                               device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d739f582-16ad-4e52-b012-429757945a84",
   "metadata": {},
   "source": [
    "## Preparing Model Inputs\n",
    "\n",
    "Before using the model, we also need to make sure our prompts are properly formatted to work with NuExtract. NuExtract expects all input information to come as a single user chat prompt, formatted as follows:\n",
    "\n",
    "```python\n",
    "f\"\"\"\n",
    "# Template:\n",
    "{template}\n",
    "# Context:\n",
    "{text}\n",
    "\"\"\"\n",
    "```\n",
    "and if in-context examples are provided:\n",
    "```python\n",
    "f\"\"\"\n",
    "# Template:\n",
    "{template}\n",
    "# Examples\n",
    "## Input:\n",
    "{input1}\n",
    "## Output:\n",
    "{output1}\n",
    "## Input:\n",
    "{input2}\n",
    "## Output:\n",
    "{output2}\n",
    "# Context:\n",
    "{text}\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "If you are working with image inputs, you should use image placeholders for `text`, `output1`, etc. Later, we will inject tokens representing the actual image content in the location of these placeholders.\n",
    "\n",
    "The following functions will make this formatting more convenient for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58b017a4-4802-4afe-b2b4-8f034f572c24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def construct_messages(document, template, examples=None, image_placeholder=\"<|vision_start|><|image_pad|><|vision_end|>\"):\n",
    "    \"\"\"\n",
    "    Construct the individual NuExtract message texts, prior to chat template formatting.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    # add few-shot examples if needed\n",
    "    if examples is not None and len(examples) > 0:\n",
    "        icl = \"# Examples:\\n\"\n",
    "        for row in examples:\n",
    "            example_input = row['input']\n",
    "            \n",
    "            if not isinstance(row['input'], str):\n",
    "                example_input = image_placeholder\n",
    "                images.append(row['input'])\n",
    "                \n",
    "            icl += f\"## Input:\\n{example_input}\\n## Output:\\n{row['output']}\\n\"\n",
    "    else:\n",
    "        icl = \"\"\n",
    "        \n",
    "    # if input document is an image, set text to an image placeholder\n",
    "    text = document\n",
    "    if not isinstance(document, str):\n",
    "        text = image_placeholder\n",
    "        images.append(document)\n",
    "    text = f\"\"\"# Template:\\n{template}\\n{icl}# Context:\\n{text}\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": text}] + images,\n",
    "        }\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e918d64e-bc31-4d68-8ef5-2d485b5e5745",
   "metadata": {},
   "source": [
    "## Inference\n",
    "### Basic Example\n",
    "\n",
    "Now we are ready to run the model!\n",
    "\n",
    "Let's start with a basic text-only example, where we want to extract peoples' names from a short text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1cad1ec-4753-4064-aca0-c89fec4b5bfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "template = \"\"\"{\"names\": [\"verbatim-string]}\"\"\"\n",
    "document = \"John went to the restaurant with Mary. James went to the cinema.\"\n",
    "\n",
    "# prepare the user message content\n",
    "messages = construct_messages(document, template)\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "image_inputs = process_vision_info(messages)[0]\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0e4f92-94dd-4b37-8772-591246f9b71a",
   "metadata": {},
   "source": [
    "Our NuExtract message is now formatted in standard chat template formatting; the tokenized version (`inputs`) will be given directly to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42bd5add-b48a-44e4-9843-2f187e768678",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "# Template:\n",
      "{\"names\": [\"verbatim-string]}\n",
      "# Context:\n",
      "John went to the restaurant with Mary. James went to the cinema.<|im_end|> \n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0979a7-ea46-4d25-a838-baeaeeca6ee5",
   "metadata": {},
   "source": [
    "The other `image_inputs` are empty in this case because this is a text-only example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3493013-313f-4318-a813-d492732bb0c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(image_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e300775-48a9-42fb-9c92-3e1ceb530101",
   "metadata": {},
   "source": [
    "Now let's actually run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed374bf3-1af1-4a3e-a9eb-9e2e7ca113b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1e-06` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"names\": [\"John\", \"Mary\", \"James\"]}']\n"
     ]
    }
   ],
   "source": [
    "# we choose greedy sampling here, which works well for most information extraction tasks\n",
    "generation_config = {\"do_sample\": False, \"num_beams\": 1, \"max_new_tokens\": 2048}\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    **generation_config\n",
    ")\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18002f2-447a-45c2-90f6-0ff5a63c5dc2",
   "metadata": {},
   "source": [
    "### In-Context Examples\n",
    "\n",
    "Sometimes the model might not perform as well as we want because our task is challenging or involves some degree of ambiguity. Alternatively, we may want the model to follow some specific formatting, or just give it a bit more help. In cases like this it can be valuable to provide \"in-context examples\" to help NuExtract better understand the task.\n",
    "\n",
    "To do so, we can provide a list `examples` to `construct_messages` which contain dictionaries of input/output pairs. In the example below, we show to the model that we want the extracted names to be in captial letters with `++` on either side (for the sake of illustration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "146baa04-2206-4eca-86bd-66f94924e293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"{\"names\": [\"verbatim-string\"]}\"\"\"\n",
    "text = \"John went to the restaurant with Mary. James went to the cinema.\"\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Stephen is the manager at Susan's store.\",\n",
    "        \"output\": \"\"\"{\"names\": [\"++STEPHEN++\", \"++SUSAN++\"]}\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "messages = construct_messages(text, template, examples)\n",
    "\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "image_inputs = process_vision_info(messages)[0]\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2561cc-a662-4898-b226-41d12394fdb4",
   "metadata": {},
   "source": [
    "We can see below that the in-context example has now been included in the model prompt, specifically between the template and context components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79b10e44-5bb4-40eb-af74-e013897948ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "# Template:\n",
      "{\"names\": [\"verbatim-string\"]}\n",
      "# Examples:\n",
      "## Input:\n",
      "Stephen is the manager at Susan's store.\n",
      "## Output:\n",
      "{\"names\": [\"++STEPHEN++\", \"++SUSAN++\"]}\n",
      "# Context:\n",
      "John went to the restaurant with Mary. James went to the cinema.<|im_end|> \n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f1364bf-2588-4935-a478-7a85b87866fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"names\": [\"++JOHN++\", \"++MARY++\", \"++JAMES++\"]}']\n"
     ]
    }
   ],
   "source": [
    "# we choose greedy sampling here, which works well for most information extraction tasks\n",
    "generation_config = {\"do_sample\": False, \"num_beams\": 1, \"max_new_tokens\": 2048}\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    **generation_config\n",
    ")\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0c3ecf-80ee-40b7-b240-12b5a9e10042",
   "metadata": {},
   "source": [
    "To get even better performance, add multiple in-context examples to your input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99faf117-18b7-4b5e-8cd0-b6f8941db581",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Image Inputs\n",
    "\n",
    "If we want to give image inputs to NuExtract, instead of text, we simply provide a dictionary specifying the desired image file as the message content, instead of a string. E.g. `{\"type\": \"image\", \"image\": \"file://image.jpg\"}`.\n",
    "\n",
    "You can also specify an image URL (e.g. `{\"type\": \"image\", \"image\": \"http://path/to/your/image.jpg\"}`) or base64 encoding (e.g. `{\"type\": \"image\", \"image\": \"data:image;base64,/9j/...\"}`).\n",
    "\n",
    "In the example below, we give an image of a receipt (`1.jpg`) and ask the model to extract the name of the store. We also provide an ICL example of a receipt from Walmart (`0.jpg`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd11e855-594b-4fd7-af01-7695a65b75a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"{\"store\": \"verbatim-string\"}\"\"\"\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": {\"type\": \"image\", \"image\": \"file://0.jpg\"},\n",
    "        \"output\": \"\"\"{\"store\": \"WALMART\"}\"\"\"\n",
    "    }\n",
    "]\n",
    "document = {\"type\": \"image\", \"image\": \"file://1.jpg\"}\n",
    "\n",
    "messages = construct_messages(document, template, examples)\n",
    "\n",
    "text = processor.tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "image_inputs = process_vision_info(messages)[0]\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14a8e8a-a091-410a-a964-9d275b936e75",
   "metadata": {},
   "source": [
    "Just like in the text-only case above, our in-context example has been included in the prompt before the main context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba00c8d6-56e2-43fc-a646-61f693c6b51a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "# Template:\n",
      "{\"store\": \"verbatim-string\"}\n",
      "# Examples:\n",
      "## Input:\n",
      "<|vision_start|><|image_pad|><|vision_end|>\n",
      "## Output:\n",
      "{\"store\": \"WALMART\"}\n",
      "# Context:\n",
      "<|vision_start|><|image_pad|><|vision_end|><|im_end|> \n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4820e6da-544b-431a-86fb-a6f9f0eb6bdd",
   "metadata": {},
   "source": [
    "Now if we look at `image_inputs` we will see that it contains actual images. When we pass this along with `text` to the `processor` it will automatically encode the images and inject a tokenized representation into the image placeholders within `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aa9fba6-eb58-4401-abc2-79b7fcd205c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<PIL.Image.Image image mode=RGB size=588x896 at 0x7FD1CCFF1A20>, <PIL.Image.Image image mode=RGB size=476x980 at 0x7FD1CCFF1750>]\n"
     ]
    }
   ],
   "source": [
    "print(image_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbed800c-519b-4ee1-90d9-7c27e383d904",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"store\": \"TRADER JOE\\'S\"}']\n"
     ]
    }
   ],
   "source": [
    "generation_config = {\"do_sample\": False, \"num_beams\": 1, \"max_new_tokens\": 2048}\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    **generation_config\n",
    ")\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6beb77f-2de5-4846-b244-2cda27f5f553",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Batched Inference\n",
    "\n",
    "Finally, we can run batched inference over a list of input examples, regardless of whether they contain text, images, and/or ICL examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "947b8090-236c-4821-b295-5a8fb5247943",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"store_name\": \"WAL-MART\"}\n",
      "{\"store_name\": \"Trader Joe's\"}\n",
      "{\"names\": [\"John\", \"Mary\", \"James\"]}\n",
      "{\"names\": [\"JOHN\", \"MARY\", \"JAMES\"]}\n"
     ]
    }
   ],
   "source": [
    "inputs = [\n",
    "    # image input with no ICL examples\n",
    "    {\n",
    "        \"document\": {\"type\": \"image\", \"image\": \"file://0.jpg\"},\n",
    "        \"template\": \"\"\"{\"store_name\": \"verbatim-string\"}\"\"\",\n",
    "    },\n",
    "    # image input with 1 ICL example\n",
    "    {\n",
    "        \"document\": {\"type\": \"image\", \"image\": \"file://1.jpg\"},\n",
    "        \"template\": \"\"\"{\"store_name\": \"verbatim-string\"}\"\"\",\n",
    "        \"examples\": [\n",
    "            {\n",
    "                \"input\": {\"type\": \"image\", \"image\": \"file://0.jpg\"},\n",
    "                \"output\": \"\"\"{\"store_name\": \"Walmart\"}\"\"\",\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    # text input with no ICL examples\n",
    "    {\n",
    "        \"document\": \"John went to the restaurant with Mary. James went to the cinema.\",\n",
    "        \"template\": \"\"\"{\"names\": [\"verbatim-string\"]}\"\"\",\n",
    "    },\n",
    "    # text input with ICL example\n",
    "    {\n",
    "        \"document\": \"John went to the restaurant with Mary. James went to the cinema.\",\n",
    "        \"template\": \"\"\"{\"names\": [\"verbatim-string\"]}\"\"\",\n",
    "        \"examples\": [\n",
    "            {\n",
    "                \"input\": \"Stephen is the manager at Susan's store.\",\n",
    "                \"output\": \"\"\"{\"names\": [\"STEPHEN\", \"SUSAN\"]}\"\"\"\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "messages = [\n",
    "    construct_messages(\n",
    "        x[\"document\"], \n",
    "        x[\"template\"], \n",
    "        x[\"examples\"] if \"examples\" in x else None\n",
    "    ) for x in inputs\n",
    "]\n",
    "\n",
    "# apply chat template to each example individually\n",
    "texts = [\n",
    "    processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n",
    "    for msg in messages\n",
    "]\n",
    "\n",
    "image_inputs= process_vision_info(messages)[0]\n",
    "inputs = processor(\n",
    "    text=texts,\n",
    "    images=image_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "generation_config = {\"do_sample\": False, \"num_beams\": 1, \"max_new_tokens\": 2048}\n",
    "\n",
    "# Batch Inference\n",
    "generated_ids = model.generate(**inputs, **generation_config)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_texts = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "for y in output_texts:\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf456dcc-52ce-495d-872f-a641c6f40f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
