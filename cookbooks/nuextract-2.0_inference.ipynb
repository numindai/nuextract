{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbe8b1f6-ec66-4fbc-a9f8-774acd8a255a",
   "metadata": {},
   "source": [
    "# NuExtract 2.0 Inference\n",
    "\n",
    "In this notebook we will provide examples of how to use the NuExtract 2.0 models for inference.\n",
    "\n",
    "First, let's load a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a561e38e-58e5-4f7e-80ee-eabc2d05e49a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "model_name = \"numind/NuExtract-2.0-2B\"\n",
    "# model_name = \"numind/NuExtract-2.0-8B\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name, \n",
    "                                          trust_remote_code=True, \n",
    "                                          padding_side='left',\n",
    "                                          use_fast=True)\n",
    "model = AutoModelForVision2Seq.from_pretrained(model_name, \n",
    "                                               trust_remote_code=True, \n",
    "                                               torch_dtype=torch.bfloat16,\n",
    "                                               attn_implementation=\"flash_attention_2\",\n",
    "                                               device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d739f582-16ad-4e52-b012-429757945a84",
   "metadata": {},
   "source": [
    "## Preparing Model Inputs\n",
    "\n",
    "Before using the model, we also need to make sure our prompts are properly formatted to work with NuExtract. NuExtract expects all input information to come as a single user chat prompt, formatted as follows:\n",
    "\n",
    "```python\n",
    "f\"\"\"\n",
    "# Template:\n",
    "{template}\n",
    "# Context:\n",
    "{text}\n",
    "\"\"\"\n",
    "```\n",
    "and if in-context examples are provided:\n",
    "```python\n",
    "f\"\"\"\n",
    "# Template:\n",
    "{template}\n",
    "# Examples\n",
    "## Input:\n",
    "{input1}\n",
    "## Output:\n",
    "{output1}\n",
    "## Input:\n",
    "{input2}\n",
    "## Output:\n",
    "{output2}\n",
    "# Context:\n",
    "{text}\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "If you are working with image inputs, you should use image placeholders for `text`, `input1`, etc. Later, we will inject tokens representing the actual image content in the location of these placeholders.\n",
    "\n",
    "The following function will make this formatting more convenient for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b017a4-4802-4afe-b2b4-8f034f572c24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def construct_messages(document, template, examples=None, image_placeholder=\"<|vision_start|><|image_pad|><|vision_end|>\"):\n",
    "    \"\"\"\n",
    "    Construct the individual NuExtract message texts, prior to chat template formatting.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    # add few-shot examples if needed\n",
    "    if examples is not None and len(examples) > 0:\n",
    "        icl = \"# Examples:\\n\"\n",
    "        for row in examples:\n",
    "            example_input = row['input']\n",
    "            \n",
    "            if not isinstance(row['input'], str):\n",
    "                example_input = image_placeholder\n",
    "                images.append(row['input'])\n",
    "                \n",
    "            icl += f\"## Input:\\n{example_input}\\n## Output:\\n{row['output']}\\n\"\n",
    "    else:\n",
    "        icl = \"\"\n",
    "        \n",
    "    # if input document is an image, set text to an image placeholder\n",
    "    text = document\n",
    "    if not isinstance(document, str):\n",
    "        text = image_placeholder\n",
    "        images.append(document)\n",
    "    text = f\"\"\"# Template:\\n{template}\\n{icl}# Context:\\n{text}\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are NuExtract, an information extraction tool created by NuMind.\" \n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": text}] + images,\n",
    "        }\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e918d64e-bc31-4d68-8ef5-2d485b5e5745",
   "metadata": {},
   "source": [
    "## Inference\n",
    "### Basic Example\n",
    "\n",
    "Now we are ready to run the model!\n",
    "\n",
    "Let's start with a basic text-only example, where we want to extract peoples' names from a short text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cad1ec-4753-4064-aca0-c89fec4b5bfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "template = \"\"\"{\"names\": [\"verbatim-string\"]}\"\"\"\n",
    "document = \"John went to the restaurant with Mary. James went to the cinema.\"\n",
    "\n",
    "# prepare the user message content\n",
    "messages = construct_messages(document, template)\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "image_inputs = process_vision_info(messages)[0]\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0e4f92-94dd-4b37-8772-591246f9b71a",
   "metadata": {},
   "source": [
    "Our NuExtract message is now formatted in standard chat template formatting; the tokenized version (`inputs`) will be given directly to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42bd5add-b48a-44e4-9843-2f187e768678",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "# Template:\n",
      "{\"names\": [\"string\"]}\n",
      "# Context:\n",
      "John went to the restaurant with Mary. James went to the cinema.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0979a7-ea46-4d25-a838-baeaeeca6ee5",
   "metadata": {},
   "source": [
    "The other `image_inputs` are empty in this case because this is a text-only example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3493013-313f-4318-a813-d492732bb0c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(image_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e300775-48a9-42fb-9c92-3e1ceb530101",
   "metadata": {},
   "source": [
    "Now let's actually run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed374bf3-1af1-4a3e-a9eb-9e2e7ca113b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.001` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"names\": [\"John\", \"Mary\", \"James\"]}']\n"
     ]
    }
   ],
   "source": [
    "# we choose greedy sampling here, which works well for most information extraction tasks\n",
    "generation_config = {\"do_sample\": False, \"num_beams\": 1, \"max_new_tokens\": 2048}\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    **generation_config\n",
    ")\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d37d33-85ad-421c-a014-523ddbec5a2f",
   "metadata": {},
   "source": [
    "Alternatively, you can directly provide the template and in-context examples to `.apply_chat_template()`, rather then manually preparing the prompt via `construct_messages()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fecb60-b0eb-46bc-ba2d-ef6a32d04f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "# Template:\n",
      "{\"names\": [\"string\"]}\n",
      "# Context:\n",
      "John went to the restaurant with Mary. James went to the cinema.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "['{\"names\": [\"John\", \"Mary\", \"James\"]}']\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"{\"names\": [\"verbatim-string\"]}\"\"\"\n",
    "document = \"John went to the restaurant with Mary. James went to the cinema.\"\n",
    "\n",
    "# prepare the user message content\n",
    "messages = [{\"role\": \"user\", \"content\": document}]\n",
    "text = processor.tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    template=template, # template is specified here\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "print(text)\n",
    "\n",
    "image_inputs = process_vision_info(messages)[0]\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "# we choose greedy sampling here, which works well for most information extraction tasks\n",
    "generation_config = {\"do_sample\": False, \"num_beams\": 1, \"max_new_tokens\": 2048}\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    **generation_config\n",
    ")\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18002f2-447a-45c2-90f6-0ff5a63c5dc2",
   "metadata": {},
   "source": [
    "### In-Context Examples\n",
    "\n",
    "Sometimes the model might not perform as well as we want because our task is challenging or involves some degree of ambiguity. Alternatively, we may want the model to follow some specific formatting, or just give it a bit more help. In cases like this it can be valuable to provide \"in-context examples\" to help NuExtract better understand the task.\n",
    "\n",
    "To do so, we can provide a list `examples` to `apply_chat_template()` (or `construct_messages()`) which contains dictionaries of input/output pairs. In the example below, we show to the model that we want the extracted names to be in captial letters with `-` on either side (for the sake of illustration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146baa04-2206-4eca-86bd-66f94924e293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"{\"names\": [\"verbatim-string\"]}\"\"\"\n",
    "document = \"John went to the restaurant with Mary. James went to the cinema.\"\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Stephen is the manager at Susan's store.\",\n",
    "        \"output\": \"\"\"{\"names\": [\"-STEPHEN-\", \"-SUSAN-\"]}\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": document}]\n",
    "text = processor.tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    template=template,\n",
    "    examples=examples, # examples provided here\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "image_inputs = process_vision_info(messages)[0]\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2561cc-a662-4898-b226-41d12394fdb4",
   "metadata": {},
   "source": [
    "We can see below that the in-context example has now been included in the model prompt, specifically between the template and context components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79b10e44-5bb4-40eb-af74-e013897948ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "# Template:\n",
      "{\"names\": [\"string\"]}\n",
      "# Examples:\n",
      "## Input:\n",
      "Stephen is the manager at Susan's store.\n",
      "## Output:\n",
      "{\"names\": [\"-STEPHEN-\", \"-SUSAN-\"]}\n",
      "# Context:\n",
      "John went to the restaurant with Mary. James went to the cinema.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f1364bf-2588-4935-a478-7a85b87866fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"names\": [\"-JOHN-\", \"-MARY-\", \"-JAMES-\"]}']\n"
     ]
    }
   ],
   "source": [
    "# we choose greedy sampling here, which works well for most information extraction tasks\n",
    "generation_config = {\"do_sample\": False, \"num_beams\": 1, \"max_new_tokens\": 2048}\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    **generation_config\n",
    ")\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0c3ecf-80ee-40b7-b240-12b5a9e10042",
   "metadata": {},
   "source": [
    "To get even better performance, add multiple in-context examples to your input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99faf117-18b7-4b5e-8cd0-b6f8941db581",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Image Inputs\n",
    "\n",
    "If we want to give image inputs to NuExtract, instead of text, we simply provide a dictionary specifying the desired image file as the message content, instead of a string. E.g. `{\"type\": \"image\", \"image\": \"file://image.jpg\"}`.\n",
    "\n",
    "You can also specify an image URL (e.g. `{\"type\": \"image\", \"image\": \"http://path/to/your/image.jpg\"}`) or base64 encoding (e.g. `{\"type\": \"image\", \"image\": \"data:image;base64,/9j/...\"}`).\n",
    "\n",
    "First, we will need a modified version of `process_vision_info()` that handles image-based in-context examples as well as primary inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1aaf1912-4f50-483f-a9a0-f5f57caf66e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_all_vision_info(messages, examples=None):\n",
    "    \"\"\"\n",
    "    Process vision information from both messages and in-context examples, supporting batch processing.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dictionaries (single input) OR list of message lists (batch input)\n",
    "        examples: Optional list of example dictionaries (single input) OR list of example lists (batch)\n",
    "    \n",
    "    Returns:\n",
    "        A flat list of all images in the correct order:\n",
    "        - For single input: example images followed by message images\n",
    "        - For batch input: interleaved as (item1 examples, item1 input, item2 examples, item2 input, etc.)\n",
    "        - Returns None if no images were found\n",
    "    \"\"\"\n",
    "    from qwen_vl_utils import process_vision_info, fetch_image\n",
    "    \n",
    "    # Helper function to extract images from examples\n",
    "    def extract_example_images(example_item):\n",
    "        if not example_item:\n",
    "            return []\n",
    "            \n",
    "        # Handle both list of examples and single example\n",
    "        examples_to_process = example_item if isinstance(example_item, list) else [example_item]\n",
    "        images = []\n",
    "        \n",
    "        for example in examples_to_process:\n",
    "            if isinstance(example.get('input'), dict) and example['input'].get('type') == 'image':\n",
    "                images.append(fetch_image(example['input']))\n",
    "                \n",
    "        return images\n",
    "    \n",
    "    # Normalize inputs to always be batched format\n",
    "    is_batch = messages and isinstance(messages[0], list)\n",
    "    messages_batch = messages if is_batch else [messages]\n",
    "    is_batch_examples = examples and isinstance(examples, list) and (isinstance(examples[0], list) or examples[0] is None)\n",
    "    examples_batch = examples if is_batch_examples else ([examples] if examples is not None else None)\n",
    "    \n",
    "    # Ensure examples batch matches messages batch if provided\n",
    "    if examples and len(examples_batch) != len(messages_batch):\n",
    "        if not is_batch and len(examples_batch) == 1:\n",
    "            # Single example set for a single input is fine\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"Examples batch length must match messages batch length\")\n",
    "    \n",
    "    # Process all inputs, maintaining correct order\n",
    "    all_images = []\n",
    "    for i, message_group in enumerate(messages_batch):\n",
    "        # Get example images for this input\n",
    "        if examples and i < len(examples_batch):\n",
    "            input_example_images = extract_example_images(examples_batch[i])\n",
    "            all_images.extend(input_example_images)\n",
    "        \n",
    "        # Get message images for this input\n",
    "        input_message_images = process_vision_info(message_group)[0] or []\n",
    "        all_images.extend(input_message_images)\n",
    "    \n",
    "    return all_images if all_images else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db9904c-451d-4ff4-b776-5a557abc1437",
   "metadata": {},
   "source": [
    "In the example below, we give an image of a receipt (`data/1.jpg`) and ask the model to extract the name of the store. We also provide an ICL example of a receipt from Walmart (`data/0.jpg`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd11e855-594b-4fd7-af01-7695a65b75a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"{\"store\": \"verbatim-string\"}\"\"\"\n",
    "document = {\"type\": \"image\", \"image\": \"file://data/1.jpg\"}\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": {\"type\": \"image\", \"image\": \"file://data/0.jpg\"},\n",
    "        \"output\": \"\"\"{\"store\": \"WALMART\"}\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [document]}]\n",
    "text = processor.tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    template=template,\n",
    "    examples=examples,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "image_inputs = process_all_vision_info(messages, examples)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14a8e8a-a091-410a-a964-9d275b936e75",
   "metadata": {},
   "source": [
    "Just like in the text-only case above, our in-context example has been included in the prompt before the main context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba00c8d6-56e2-43fc-a646-61f693c6b51a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "# Template:\n",
      "{\"store\": \"verbatim-string\"}\n",
      "# Examples:\n",
      "## Input:\n",
      "<|vision_start|><|image_pad|><|vision_end|>\n",
      "## Output:\n",
      "{\"store\": \"WALMART\"}\n",
      "# Context:\n",
      "<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4820e6da-544b-431a-86fb-a6f9f0eb6bdd",
   "metadata": {},
   "source": [
    "Now if we look at `image_inputs` we will see that it contains actual images. When we pass this along with `text` to `processor()` it automatically encodes the images and injects a tokenized representation into the image placeholders within `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1aa9fba6-eb58-4401-abc2-79b7fcd205c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<PIL.Image.Image image mode=RGB size=588x896 at 0x7F2E59587760>, <PIL.Image.Image image mode=RGB size=476x980 at 0x7F2E595867D0>]\n"
     ]
    }
   ],
   "source": [
    "print(image_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbed800c-519b-4ee1-90d9-7c27e383d904",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"store\": \"TRADER JOE\\'S\"}']\n"
     ]
    }
   ],
   "source": [
    "generation_config = {\"do_sample\": False, \"num_beams\": 1, \"max_new_tokens\": 2048}\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    **generation_config\n",
    ")\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6beb77f-2de5-4846-b244-2cda27f5f553",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Batched Inference\n",
    "\n",
    "Finally, we can run batched inference over a list of input examples, regardless of whether they contain text, images, and/or ICL examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947b8090-236c-4821-b295-5a8fb5247943",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"store_name\": \"WAL-MART\"}\n",
      "{\"store_name\": \"Walmart\"}\n",
      "{\"names\": [\"John\", \"Mary\", \"James\"]}\n",
      "{\"names\": [\"-JOHN-\", \"-MARY-\", \"-JAMES-\"]}\n"
     ]
    }
   ],
   "source": [
    "inputs = [\n",
    "    # image input with no ICL examples\n",
    "    {\n",
    "        \"document\": {\"type\": \"image\", \"image\": \"file://data/0.jpg\"},\n",
    "        \"template\": \"\"\"{\"store_name\": \"verbatim-string\"}\"\"\",\n",
    "    },\n",
    "    # image input with 1 ICL example\n",
    "    {\n",
    "        \"document\": {\"type\": \"image\", \"image\": \"file://data/0.jpg\"},\n",
    "        \"template\": \"\"\"{\"store_name\": \"verbatim-string\"}\"\"\",\n",
    "        \"examples\": [\n",
    "            {\n",
    "                \"input\": {\"type\": \"image\", \"image\": \"file://data/1.jpg\"},\n",
    "                \"output\": \"\"\"{\"store_name\": \"Trader Joe's\"}\"\"\",\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    # text input with no ICL examples\n",
    "    {\n",
    "        \"document\": {\"type\": \"text\", \"text\": \"John went to the restaurant with Mary. James went to the cinema.\"},\n",
    "        \"template\": \"\"\"{\"names\": [\"verbatim-string\"]}\"\"\",\n",
    "    },\n",
    "    # text input with ICL example\n",
    "    {\n",
    "        \"document\": {\"type\": \"text\", \"text\": \"John went to the restaurant with Mary. James went to the cinema.\"},\n",
    "        \"template\": \"\"\"{\"names\": [\"verbatim-string\"]}\"\"\",\n",
    "        \"examples\": [\n",
    "            {\n",
    "                \"input\": \"Stephen is the manager at Susan's store.\",\n",
    "                \"output\": \"\"\"{\"names\": [\"-STEPHEN-\", \"-SUSAN-\"]}\"\"\"\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# messages should be a list of lists for batch processing\n",
    "messages = [[{\"role\": \"user\", \"content\": [x['document']]}] for x in inputs]\n",
    "\n",
    "# apply chat template to each example individually\n",
    "texts = [\n",
    "    processor.tokenizer.apply_chat_template(\n",
    "        messages[i],  # Now this is a list containing one message\n",
    "        template=x['template'],\n",
    "        examples=x.get('examples', None),\n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True)\n",
    "    for i, x in enumerate(inputs)\n",
    "]\n",
    "\n",
    "image_inputs = process_all_vision_info(messages, [x.get('examples') for x in inputs])\n",
    "inputs = processor(\n",
    "    text=texts,\n",
    "    images=image_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "generation_config = {\"do_sample\": False, \"num_beams\": 1, \"max_new_tokens\": 2048}\n",
    "\n",
    "# Batch Inference\n",
    "generated_ids = model.generate(**inputs, **generation_config)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_texts = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "for y in output_texts:\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf456dcc-52ce-495d-872f-a641c6f40f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
